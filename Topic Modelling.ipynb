{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#  Introduction to Topic Modelling, the TL;DR you need in your life\n",
    "## Theoretical Context and the Dirichlet Distribution\n",
    "Topic modelling is an extremely powerful tool used to identify the hidden thematic structure in a set of documents. By assuming that every word in a document is generated from a fixed set of topics, we can provide insight on the latent structure of these documents. There are differnent was of doing this. Probabilistic approaches - one of which we will explore today, as well as matrix factorization approaches (non-negative matrix approximations), which we won't be covering.\n",
    "\n",
    "We will explore Latent Dirichlet Allocation, a specific probabilistic approach to topic modelling. Before we get into LDA, let's take a step back to review the Dirichlet distribution. LDA makes several assumptions which we will mention as we go through this module, but there is one we can bring up now: LDA assumes a bag of words model! Meaning that we don't really care about dependencies between words, or dependencies between documents. \n",
    "\n",
    "Recall:\n",
    "\n",
    "A standard probability distribution used on the simplex is the *Dirichlet distribution*. The Dirichlet distribution can be defined as follows. \n",
    "\n",
    "$$p\\left(\\theta_{1},\\ldots ,\\theta_{K};\\alpha _{1},\\ldots ,\\alpha _{K}\\right)= \\frac{1}{B(\\vec{\\alpha})}\\prod _{i=1}^{K}\\theta_{i}^{\\alpha _{i}-1}$$\n",
    "\n",
    "Where $B(\\vec{\\alpha})$ is just the normalizing constant that makes the distribution sum to (actually integrate to) $1$. \n",
    "\n",
    "\n",
    "### What can we do with topic modelling?\n",
    "It might be of interest to examine how trends in documents change over time. For example, a study conducted at Brown University by Uriel Cohen Priva,Ph.D and Joseph Austerweil,Ph.D analyzed a large dataset of articles from the journal Cognition. They wanted to explore trends within Cognition over four decades. They found several trends in the data, like \"the rise of moral cognition, eyetrackig methods\", etc.\n",
    "\n",
    "Fun fact: before its applications in computational linguistics, topic models were used in image processing, as well as in the processing of biological data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formalization: Topic Modelling Algorithm \n",
    "\n",
    "Ian: this needs more work, all i did was copy + paste what was in the outline. Feel free to add any more pictures and definitely more discussion, whatever u think is best.\n",
    "\n",
    "Discuss how we use topic models to see how topics (rather than words)\n",
    "trend over time.\n",
    "iii.\n",
    "Broad process definition: topic models assume that every word in a\n",
    "document is generated by one of a # of topics.\n",
    "1. A distribution over topics (the gist of a document) is sampled from\n",
    "a dirichlet\n",
    "2. Each word is sampled from the topics of a document\n",
    "3. Documents are biased to be more likely to generate some words\n",
    "rather than others\n",
    "4. Together these biases lead the model when given a corpus of\n",
    "documents to converge on solutions in which words that are likely\n",
    "to co-occur are generated by the same topic!\n",
    "iv.\n",
    "Explain the process with a whiteboard demo??\n",
    "\n",
    "\n",
    "<img src=\"topicmodelwords.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation: An Approach to Topic Modelling \n",
    "note to graham: definitely explain the assumptions LDA makes (documents independent,words independent, fixed number of topics assumed)\n",
    "\n",
    "Recall that the hierarchical BOW model uses 2 parameters: a vocabulary and some distribution over thetas (thetas denoting distributions over the vocabulary). The distribution over thetas that we covered was the Dirichlet distribution, which took a pseudocount vector that corresponded to the vocabulary and sampled the theta that would be used to sample every word in the corpus. \n",
    "\n",
    "Sampling from the hierarchical BOW model is a two step process: <ol><li> Sample a theta from the Dirichlet distribution. </li> <li>Use that theta to independently sample words from the vocabulary repeatedly until you reach the desired length of your corpus.</li></ol> \n",
    "\n",
    "Let's first go over what we will define as a topic. A <i>topic</i> is a distribution over the vocabulary of the set of documents, akin to the theta used in the hierarchical BOW model. One could also think of the hierarchical BOW model as a document generated by exactly one topic.\n",
    "\n",
    "One can think of the intuition behind this as the meaning of a topic comes from the use of related words. In a document that focuses on a collection of topics, you might notice that words related to these topics appear more often than words related to other topics. So if you have a document about the stock market, you will get words like <i>firms</i>, <i>price</i>, <i>corporate</i>, and <i>value</i> more often than a document about politics, which could have words like <i>constitutional</i>, <i>government</i>, <i>justice</i>, and <i>amendment</i>. Of course, there could be an overlap in the words of each document, but the idea is each document will have a difference in distribution over words depending on the topics that it focuses on. If articles can be annotated by their topics rather than by title or existence of keywords, we can bypass the homograph problem and develop stronger searching and categorization methods.\n",
    "\n",
    "\n",
    "Latent Dirichlet Allocation (LDA) is another generative model which generates a set of documents as opposed to a single corpus. It makes a number of assumptions: <ul> <li>There are a set number of predefined topics. We do this to specify the words that will be sampled more often. We could relax this constraint to include every possible distribution over the vocabulary in our list of topics, but there are many possible topics that don't abstract to anything substantial, such as the uniform distribution. It is up to the designer to determine what distribution a topic represents.</li> <li>Each document is generated by multiple topics (as opposed to the BOW hierarchical model which would assume only one topic for the whole corpus).</li> <li>The distributions over topics are assigned to each document independently.</li> <li>Each topic is assigned to each word independently.</li> <li>Each word is sampled independently.</li></ul>\n",
    "\n",
    "Latent Dirichlet Allocation generates multiple documents independently using a 3 step process: <ol><li>For the each document, sample a distribution over the set of topics to use for the document.</li> <li> For the <i>i</i>th word in a document, sample a topic from the distribution to assign to the word. </li><li> Sample a word from that topic to be the <i>i</i>th word in the document.</li></ol> Refer to the diagrams below for a visual comparison of the LDA model (pictured left) and the BOW hierarchical model (pictured right). \n",
    "\n",
    "\n",
    "\n",
    "<img src=\"gist.png\" style=\"float: left; width: 30%; margin-right: 1%; margin-bottom: 0.5em;\">\n",
    "<img src=\"graphical-model.png\" style=\"float: left; width: 20%; margin-right: 1%; margin-bottom: 0.5em;\">\n",
    "<p style=\"clear: both;\">\n",
    "\n",
    "We can write out the distributions for the LDA model as follows:\n",
    "\n",
    "$$\\begin{align}\n",
    "{\\theta}_{d} &\\sim& \\mathrm{Dirichlet}(\\vec{\\alpha})\\\\\n",
    "Z_{d,n} \\mid {\\theta}_{d} &\\sim& \\mathrm{categorical}({\\theta}_{d})\\\\\n",
    "w_{d,n} \\mid {\\beta}_{1:K}, Z_{d,n} &\\sim& \\mathrm{categorical}({\\beta}_{1:K}, Z_{d,n})\\\\\n",
    "\\end{align}$$\n",
    "\n",
    "$$\\Pr(\\beta_{1:K},\\theta_{1:D},Z_{1:D},W_{1:D}) = \\prod_{i=1}^{k}\\Pr(\\beta_{i})\\prod_{d=1}^{D}\\Pr(\\theta_{d})(\\prod_{n=1}^{N}\\Pr(Z_{d,n}|\\theta_{d})\\Pr(w_{d,n}|\\beta_{1:K},Z_{d,n}))$$\n",
    "\n",
    "What does the LDA sampler generate and how close is it to what we want? Documents generated using Latent Dirichlet Allocation are just a jumble of words since each word is sampled independently, so dependencies between words are not captured and sentences are inevitably ill-formed. What we care about more is having a formalization of the idea that a document is about a set of topics, and as a result words related to these topics will occur more often in the document. Like with the hierarchical BOW model, we are creating a model that uses hidden variables in its sampling process so that we can infer the structure of these hidden variables given the observed words using an inversion of the sampling algorithm. We did this with the hierarchical BOW model in Problem Set 2, but only had to infer one theta associated with a corpus. Since LDA uses two hidden variables - the topic that generates a word and the distribution over topics in a document - the inference process is more complex. In order to infer the distribution over the set of topics for a document from the words observed in a document, we will need to use Gibb's sampling which will be discussed momentarily. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get-counts"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ";helper functions\n",
    "(define (normalize params)\n",
    "  (let ((sum (apply + params)))\n",
    "    (map (lambda (x) (/ x sum)) params)))\n",
    "\n",
    "(define (flip p)\n",
    "  (if (< (random 100) (* p 100))\n",
    "      #t\n",
    "      #f))\n",
    "\n",
    "(define (sample-categorical outcomes params)\n",
    "  (if (flip (car params))\n",
    "      (car outcomes)\n",
    "      (sample-categorical (cdr outcomes) \n",
    "                          (normalize (cdr params)))))\n",
    "\n",
    "(define (get-count obs observation-list count)\n",
    "  (if (equal? observation-list '())\n",
    "      count\n",
    "      (if (equal? obs (car observation-list))\n",
    "          (get-count obs (cdr observation-list) (+ 1 count))\n",
    "          (get-count obs (cdr observation-list) count))))\n",
    "\n",
    "(define (get-counts outcomes observation-list)\n",
    "  (define (count-obs obs)\n",
    "    (get-count obs observation-list 0))\n",
    "  (map count-obs outcomes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "document1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ";LDA and GIBBS CODE STARTS HERE\n",
    "(define (sample-document vocab topics theta-prior length)\n",
    "  (if (equal? length 0)\n",
    "      '()\n",
    "      (cons (sample-categorical vocab (sample-categorical topics theta-prior))\n",
    "            (sample-document vocab topics theta-prior (- length 1)))))\n",
    "\n",
    "(define (sample-document-LDA vocab topics topics-distribution theta-priors length)\n",
    "  (let ((theta-prior (sample-categorical theta-priors topics-distribution))) ;distribution over topics\n",
    "    (sample-document vocab topics theta-prior length)))\n",
    "\n",
    "(define my-corpus '((broccoli is good)\n",
    "                    (green onion is good)\n",
    "                    (meat and cheese is good)))\n",
    "\n",
    "(define vocabulary '(broccoli is good green onion meat and cheese))\n",
    "\n",
    "\n",
    "(define topic1 (list (/ 1 8 ) (/ 1 8 ) (/ 1 8 ) (/ 1 8 ) (/ 1 8 ) (/ 1 8 ) (/ 1 8 ) (/ 1 8 ))) ; distribution over the words\n",
    "(define topic2 (list (/ 1 8 ) (/ 1 16) (/ 3 16 ) (/ 1 8 ) (/ 1 8 ) (/ 1 8 ) (/ 1 8 ) (/ 1 8 )))\n",
    "(define topics (list topic1 topic2)) ;think of this as a vector which assigns prob 0 to all other word distributions\n",
    "\n",
    "; distribution over topic distributions. One assigned per document.\n",
    "\n",
    "(define theta-prior1 (list (/ 1 8) (/ 7 8))) ; assigned to topic1 and topic2 (and 0 to all other topics)\n",
    "(define theta-prior2 (list (/ 1 2) (/ 1 2))) ; theta-prior1 and theta-prior2 are distributions over the topics\n",
    "\n",
    "; allows us to pick which topics a document focuses on\n",
    "; we parametrize this simplex by focusing on topics whose distributions are closer to a realistic topic\n",
    "(define theta-priors (list theta-prior1 theta-prior2)) ;think of this as a simplex which puts weight on these two distributions over topics\n",
    "(define topics-distribution (list (/ 1 4) (/ 3 4)))\n",
    "\n",
    "(define document1 (sample-document-LDA vocabulary topics topics-distribution theta-priors 20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where are we now, where are we going?\n",
    "\n",
    "<img src = \"summary.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Inference Algorithms and LDA\n",
    "\n",
    "**Goal**: Why do we need inference algorithms?\n",
    "\n",
    "**Question**: We have a model. How do we figure out, given a bunch of data, the assignments of the hidden random variables?\n",
    "\n",
    "As we saw above, *LDA is purely a generative statistical model*. If our goal is to actually uncover the hidden latent structure of a corpus of documents, then we need to find a way to invert our generative model. We do this via a method of bayesian inference. There are several methods of inference, namely sampling methods or variational bayes. Today we will look at sampling methods. But before we get into the particular approach, we examine why it is that we need such methods.\n",
    "\n",
    "How might we compute the conditional distribution of the topic structure, given the observed documents? This involves (as we've seen in class before) computing the posterior distribution. \n",
    "\n",
    "\n",
    "$$\\Pr(\\beta_{1:K},\\theta_{1:D},Z_{1:D}|W_{1:D}) = {\\frac {\\Pr(\\beta_{1:K},\\theta_{1:D},Z_{1:D},W_{1:D})}{Pr(W_{1:D})}}$$\n",
    "\n",
    "* The **numerator** is the joint distribution of all of the random variables.\n",
    "* The **denominator** is the marginal probability of all of the observations; the probability of seeing the observed corupus under any particular topic model.\n",
    "\n",
    "We can't compute this directly because of the denominator. Since each of these random variables are hidden random variables, we would need to integrate (take the sum of) over all the random variables for a corpus. Remember - each of these random variables can take on numerous values, so calculating this precisely is intractable. Consider the case where you have 10 random variables, where each can take on 12 values. You would have to sum over 12^10 combination. This simply cannot be solved in polynomial time, rather it would take exponentially long. \n",
    "\n",
    "Backing up a step, let's recall what the marginal distribution was. Let's say we have 4 random variables we need to condition on. We have to try to integrate one variable at a time out of our equation, by summing over all of the other possible values the other random variables can take. \n",
    "\n",
    "\n",
    "TL;DR:\n",
    "The number of possible structures is exponentially large, it is intractable to compute. We simply cannot compute the posterior because of this nasty denominator. So instead, we approximate it. This can be done several ways, but we're going to focus on Gibbs Sampling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video controls src=\"WbFc7Rn.mp4\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gibbs Sampling: A Sampling Approach to Bayesian Inference\n",
    "\n",
    "\n",
    "**Gibbs Sampling** is a Markov Chain Monte Carlo (MCMC) sampling algorithm, where the markov chain is a sequence of random variable states. The fundamental idea of using MCMC here is that we make a separate probabilistic choice for each of the 1..k dimensions for each of our random variables, where each of these choices depends on the other k-1 dimensions. \n",
    "\n",
    "As mentioned above, the distribution to sample from isn't easy to compute. However, we are in fact able to compute the *conditional distribution* of a random variables given all the other random variables. So we compute the conditional distribution for each random variable we want to resample, given all other random variables in the model. \n",
    "\n",
    "A generic gibbs sampling algorithm, using likelihood to update our topic model:\n",
    "1. Start off with a random initial assignment. (Randomly assign each word in each document to one of K topics...)\n",
    "2. Algorithmically, find another set of random variables. \n",
    "    a. Calculate the probability of a word belonging to each topic, based on the document-topic distribution and the word-topic distribution\n",
    "    b. Reassign the word to a topic based on the probability calculated in step a \n",
    "   \n",
    "3. This will continue on to approximate the posterior distribution until we asymptotically approach (converge to)^2 the true posterior. \n",
    "\n",
    "Above, we see that the corpus and random variables cancel out. \n",
    "Now we have a ratio of how likely these two assignments are in comparison to eachother. We accept the new set with a probability proportional to their ratio. If we accept, we set R1=R2. \n",
    "and repeat the process otherwise we keep r1 and repeat. This moves along the set of random variables favoring the random variables which explain the corpus better.  \n",
    "\n",
    "(Note: Gibbs Sampling is just a way of sampling from a complicated distribution. There are many different ways of implementing it, depending on what research area you're working in / which distributions to sample from depending on your data. One way is to divide  by two distributions (one sampled previously, one newly sampled) and then accept the one that is most likely.))\n",
    "\n",
    "\n",
    "*Notes*: If you have a better assignment than a random assignment, the algorithm will converge in a lesser amount of time. Also, **convergence** here implies that we have sample values that are close enough to the same distribution as if they were sampled from the true posterior joint distribution. Essentially we have sampled each latent variable and each has been conditioned on the updated values of all other latent variables. \n",
    "\n",
    "Let's examine this process from a high level.\n",
    "<img src = \"gibbsexample1.png\">\n",
    "<img src = \"gibbsexample_2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion and Take-Aways\n",
    "\n",
    "* Topic modelling is composed of two things: a generative model (LDA,LSA,etc..) and an inference method (Gibbs Sampling, Variational Bayes)\n",
    "* Topic modelling allows us to generalize across documents, saving us a lot of (human) time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MIT Scheme",
   "language": "scheme",
   "name": "mit-scheme"
  },
  "language_info": {
   "codemirror_mode": "scheme",
   "file_extension": ".scm",
   "mimetype": "application/x-scheme",
   "name": "MIT Scheme",
   "pygments_lexer": "scheme",
   "version": "9.2.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
